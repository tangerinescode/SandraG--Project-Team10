# Reducing the false positives in cyber security


## Contents

- [Submission or project name](#submission-or-project-name)
  - [Contents](#contents)
  - [Short description](#short-description)
    - [What's the problem?](#whats-the-problem)
    - [How can technology help?](#how-can-technology-help)
    - [The idea](#the-idea)
  - [Demo video](#demo-video)
  - [The architecture](#the-architecture)
  - [Long description](#long-description)
  - [Project roadmap](#project-roadmap)
  - [Live demo](#live-demo)

## Short description

### What's the problem?

With the digital payments market soaring as the world shifts towards online and card-based
payment methods, comes a growing and pressing issue of cybersecurity and fraud which is now
becoming more common than ever. Because of this, ensuring payment fraud detection is now a
priority for all banks and financial organizations. Track historical data patterns and build a fraud
detection algorithm to prevent abnormal transactions in this scenario. 

### How can technology help?
Optimize cyber security algoritms - Technology to optimize cyber security algoritms and to reduce False positives.

### The idea

Using client data to train a machine learning model, you can decrease the amount of false positives for certain items more accurately in the future, and contribute that your clients are able to purchase what they want without the transaction being declined . I will assume for the purpose of this project that many clients will rather go to their online banking on their phone and add the product/name of the merchindise that they are about to purchase so that specific transaction dont get declined. Many merchandisers loose money when the bank reject a transaction because many of those clients dont try again and simply leave the phisical store or virtual store. Banks also loose money beacuse they dont get the fee that they can obtain by running the transaction.

## Demo video

[![Watch the video](https://raw.githubusercontent.com/Liquid-Prep/Liquid-Prep/main/images/readme/IBM-interview-video-image.png)](https://youtu.be/vOgCOoy_Bx0)

## The architecture

![Video transcription/translation app](https://developer.ibm.com/developer/tutorials/cfc-starter-kit-speech-to-text-app-example/images/cfc-covid19-remote-education-diagram-2.png)

1. The user navigates to the site and uploads a video file.
2. Watson Speech to Text processes the audio and extracts the text.
3. Watson Translation (optionally) can translate the text to the desired language.
4. The app stores the translated text as a document within Object Storage.

## Long description

[More detail is available here](./docs/DESCRIPTION.md)

## Project roadmap

The project currently does the following things.

- Feature 1
- Feature 2
- Feature 3

![Roadmap](./images/roadmap.jpg)

## Live demo

You can find a running system to test at [callforcode.mybluemix.net](http://callforcode.mybluemix.net/).

See the "long description" field in our submission (not in this repo) for the log-in credentials.
